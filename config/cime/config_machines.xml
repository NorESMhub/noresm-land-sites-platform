<?xml version="1.0"?>
<config_machines>

<machine MACH="centos7-nrec">
<DESC>
        Example port to centos7 linux system with gcc, netcdf, pnetcdf and mpich
        using modules from http://www.admin-magazine.com/HPC/Articles/Environment-Modules
</DESC>

<OS>LINUX</OS>
<COMPILERS>gnu</COMPILERS>
<MPILIBS>openmpi, mpi-serial</MPILIBS>
<CIME_OUTPUT_ROOT>$ENV{HOME}/NorESM_LandSites_Platform/data/output</CIME_OUTPUT_ROOT>
<DIN_LOC_ROOT>$ENV{HOME}/NorESM_LandSites_Platform/data/input/inputdata</DIN_LOC_ROOT>
<DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/NorESM_LandSites_Platform/data/input/inputdata/atm/datm7/GSWP3v1</DIN_LOC_ROOT_CLMFORC>
<DOUT_S_ROOT>$ENV{HOME}/NorESM_LandSites_Platform/data/archive/$CASE</DOUT_S_ROOT>
<BASELINE_ROOT>UNSET</BASELINE_ROOT>
<CCSM_CPRNC>UNSET</CCSM_CPRNC>
<GMAKE>make</GMAKE>
<GMAKE_J>1</GMAKE_J>
<BATCH_SYSTEM>none</BATCH_SYSTEM>
<SUPPORTED_BY>emerald</SUPPORTED_BY>
<MAX_TASKS_PER_NODE>1</MAX_TASKS_PER_NODE>
<MAX_MPITASKS_PER_NODE>1</MAX_MPITASKS_PER_NODE>
<PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
<mpirun mpilib="mpi-serial">
	<executable></executable>
</mpirun>
<mpirun mpilib="openmpi">
  <executable>mpirun</executable>
  <arguments>
     <arg name="ntasks"> -n {{ total_tasks }} </arg>
  </arguments>
</mpirun>
<module_system type="none"/>
   <environment_variables>
        <env name="HDF5_HOME">/usr/local/hdf5</env>
        <env name="NETCDF_PATH">/usr/local/netcdf</env>
    </environment_variables>
</machine>

<machine MACH="container">
  <DESC>
    Containerized development environment (Docker/Singularity) for CESM
  </DESC>
  <OS>LINUX</OS>
  <COMPILERS>gnu</COMPILERS>
  <MPILIBS>mpich</MPILIBS>
  <CIME_OUTPUT_ROOT>$CASEROOT/</CIME_OUTPUT_ROOT>
  <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
  <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7/GSWP3v1</DIN_LOC_ROOT_CLMFORC>
  <DOUT_S_ROOT>$ENV{HOME}/archive/$CASE</DOUT_S_ROOT>
  <GMAKE>make</GMAKE>
  <GMAKE_J>4</GMAKE_J>
  <BATCH_SYSTEM>none</BATCH_SYSTEM>
  <SUPPORTED_BY>cgd</SUPPORTED_BY>
  <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
  <MAX_MPITASKS_PER_NODE>256</MAX_MPITASKS_PER_NODE>
  <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
  <mpirun mpilib="mpich">
    <executable>mpiexec</executable>
    <arguments>
<arg name="anum_tasks"> -n {{ total_tasks }}</arg>
    </arguments>
  </mpirun>
  <module_system type="none">
  </module_system>
  <RUNDIR>$CIME_OUTPUT_ROOT/run</RUNDIR>
  <EXEROOT>$CIME_OUTPUT_ROOT/bld</EXEROOT>
  <environment_variables>
    <env name="NETCDF_PATH">/usr/local</env>
    <env name="PNETCDF_PATH">/usr/local</env>
    <env name="FPATH">/usr/lib64</env>
    <env name="CPATH">/usr/lib64</env>
  </environment_variables>
  <resource_limits>
    <resource name="RLIMIT_STACK">-1</resource>
  </resource_limits>
</machine>

<machine MACH="fram">
    <DESC>Lenovo NeXtScale M5, 32-way nodes, dual 16-core Xeon E5-2683@2.10GHz, 64 GiB per node, os is Linux, batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/users/$USER/ctsm</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/work/users/$USER/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cluster/work/users/$USER/inputdata/atm/datm7/GSWP3v1</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/users/$USER/archive/$CASE</DOUT_S_ROOT>
    <!--DOUT_L_MSROOT>UNSET</DOUT_L_MSROOT-->
    <BASELINE_ROOT>UNSET</BASELINE_ROOT>
    <CCSM_CPRNC>UNSET</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>noresmCommunity</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/cluster/software/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/cluster/software/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/cluster/software/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/cluster/software/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/cluster/software/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/cluster/software/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge">--force</command>
        <command name="load">StdEnv</command>
        <!-- djlo Deactivated THT settings -->  
        <!--command name="load">intel/2016a</command-->
        <!--command name="load">netCDF-Fortran/4.4.3-intel-2016a</command-->
        <!--command name="load">PnetCDF/1.8.1-intel-2016a</command-->
        <!--command name="load">CMake/3.5.2-intel-2016a</command-->
        <command name="load">iimpi/2019b</command>
        <command name="load">netCDF-Fortran/4.5.2-iimpi-2019b</command>
        <!--command name="load">PnetCDF/1.11.0-intel-2019a</command-->
        <command name="load">CMake/3.15.3-GCCcore-8.3.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="KMP_STACKSIZE">256M</env>
      <env name="I_MPI_EXTRA_FILESYSTEM_LIST">lustre</env>
      <env name="I_MPI_EXTRA_FILESYSTEM">on</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="saga">
    <DESC> Hewlett Packard Enterprise - CentOS Linux release 7.6.1810 (Core) </DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/users/$USER/ctsm</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/work/users/$USER/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/cluster/work/users/$USER/inputdata/atm/datm7/GSWP3v1</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/users/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>UNSET</BASELINE_ROOT>
    <CCSM_CPRNC>UNSET</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>noresmCommunity</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/cluster/installations/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/cluster/installations/lmod/lmod/init/sh</init_path>
      <cmd_path lang="python">/cluster/installations/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="load">StdEnv</command>
        <command name="load">CMake/3.15.3-GCCcore-8.3.0</command>
        <command name="load">Perl/5.30.0-GCCcore-8.3.0</command>
        <command name="load">XML-LibXML/2.0201-GCCcore-8.3.0</command>
        <command name="load">iimpi/2019b</command>
        <command name="load">netCDF-Fortran/4.5.2-iimpi-2019b</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="KMP_STACKSIZE">256M</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>
  
 </config_machines>

